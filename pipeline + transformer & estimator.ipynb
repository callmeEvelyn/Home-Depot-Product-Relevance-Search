{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a basic pipeline then explore alternative transformers and estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>relevance</th>\n",
       "      <th>search_term</th>\n",
       "      <th>product_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>100001</td>\n",
       "      <td>3.00</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>100001</td>\n",
       "      <td>2.50</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>BEHR Premium Textured DeckOver 1-gal. #SC-141 ...</td>\n",
       "      <td>100002</td>\n",
       "      <td>3.00</td>\n",
       "      <td>deck over</td>\n",
       "      <td>BEHR Premium Textured DECKOVER is an innovativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>100005</td>\n",
       "      <td>2.33</td>\n",
       "      <td>rain shower head</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>100005</td>\n",
       "      <td>2.67</td>\n",
       "      <td>shower only faucet</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      product_title  product_uid  \\\n",
       "0   2                  Simpson Strong-Tie 12-Gauge Angle       100001   \n",
       "1   3                  Simpson Strong-Tie 12-Gauge Angle       100001   \n",
       "2   9  BEHR Premium Textured DeckOver 1-gal. #SC-141 ...       100002   \n",
       "3  16  Delta Vero 1-Handle Shower Only Faucet Trim Ki...       100005   \n",
       "4  17  Delta Vero 1-Handle Shower Only Faucet Trim Ki...       100005   \n",
       "\n",
       "   relevance         search_term  \\\n",
       "0       3.00       angle bracket   \n",
       "1       2.50           l bracket   \n",
       "2       3.00           deck over   \n",
       "3       2.33    rain shower head   \n",
       "4       2.67  shower only faucet   \n",
       "\n",
       "                                 product_description  \n",
       "0  Not only do angles make joints stronger, they ...  \n",
       "1  Not only do angles make joints stronger, they ...  \n",
       "2  BEHR Premium Textured DECKOVER is an innovativ...  \n",
       "3  Update your bathroom with the Delta Vero Singl...  \n",
       "4  Update your bathroom with the Delta Vero Singl...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read data file\n",
    "df_train = pd.read_csv('data/train.csv', encoding = \"ISO-8859-1\")  \n",
    "df_test = pd.read_csv('data/test.csv', encoding = \"ISO-8859-1\")  \n",
    "df_desc = pd.read_csv('data/product_descriptions.csv')  \n",
    "\n",
    "# concat train and test data to process together\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "\n",
    "# merge description into data\n",
    "df_all = pd.merge(df_all, df_desc, how='left', on='product_uid')  \n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text processing ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check grammar? analyze part-of-speech(remove 'be' ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         title_words|        search_words|\n",
      "+--------------------+--------------------+\n",
      "|[simpson, strong-...|    [angle, bracket]|\n",
      "|[simpson, strong-...|        [l, bracket]|\n",
      "|[behr, premium, t...|        [deck, over]|\n",
      "|[delta, vero, 1-h...|[rain, shower, head]|\n",
      "|[delta, vero, 1-h...|[shower, only, fa...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "# instantiate Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# tokenize\n",
    "data = spark.createDataFrame(df_all, ['id', 'product_title', 'product_uid', 'relevance', 'search_term', 'product_description'])\n",
    "tokenizer = Tokenizer(inputCol=\"product_title\", outputCol=\"title_words\")\n",
    "data = tokenizer.transform(data)\n",
    "tokenizer = Tokenizer(inputCol=\"search_term\", outputCol=\"search_words\")\n",
    "data = tokenizer.transform(data)\n",
    "\n",
    "# hashingTF = HashingTF(inputCol=\"title_words\", outputCol=\"title_features\", numFeatures=10)\n",
    "# train_title_tf = hashingTF.transform(train_search)\n",
    "\n",
    "data.select(['title_words', 'search_words']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word processing ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemming?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature: title matches searching query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|match_title|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          1|\n",
      "|          2|\n",
      "|          3|\n",
      "|          6|\n",
      "|          2|\n",
      "|          4|\n",
      "|          1|\n",
      "|          4|\n",
      "|          5|\n",
      "|          1|\n",
      "|          8|\n",
      "|          3|\n",
      "|          2|\n",
      "|          8|\n",
      "|          1|\n",
      "|          4|\n",
      "|          0|\n",
      "|          0|\n",
      "|          0|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType,StringType,IntegerType\n",
    "\n",
    "#get the number of match words in term and title\n",
    "def matchWords(title,term):\n",
    "    l1=len(title)\n",
    "    l2=len(term)\n",
    "    match=0\n",
    "    for i in range(l1):\n",
    "        for j in range(l2):\n",
    "            if title[i] == term[j]:\n",
    "                match+=2\n",
    "            elif title[i] in term[j]:\n",
    "                match+=1\n",
    "            elif term[j] in title[i]:\n",
    "                match+=1\n",
    "    return match\n",
    "matchUDF=udf(matchWords, IntegerType())\n",
    "\n",
    "data = data.withColumn(\"match_title\", matchUDF(\"title_words\",\"search_words\"))\n",
    "data.select('match_title').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|features|relevance|\n",
      "+--------+---------+\n",
      "|   [2.0]|      3.0|\n",
      "|   [1.0]|      2.5|\n",
      "|   [2.0]|      3.0|\n",
      "|   [3.0]|     2.33|\n",
      "|   [6.0]|     2.67|\n",
      "|   [2.0]|      3.0|\n",
      "|   [4.0]|     2.67|\n",
      "|   [1.0]|      3.0|\n",
      "|   [4.0]|     2.67|\n",
      "|   [5.0]|      3.0|\n",
      "|   [1.0]|     2.67|\n",
      "|   [8.0]|      3.0|\n",
      "|   [3.0]|      3.0|\n",
      "|   [2.0]|      2.0|\n",
      "|   [8.0]|     2.67|\n",
      "|   [1.0]|     2.67|\n",
      "|   [4.0]|      3.0|\n",
      "|   [0.0]|      1.0|\n",
      "|   [0.0]|     1.67|\n",
      "|   [0.0]|     2.33|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# format features\n",
    "features=[\"match_title\"]\n",
    "assembler_features = VectorAssembler(inputCols=features, outputCol='features')\n",
    "data = assembler_features.transform(data)\n",
    "data.select('features', 'relevance').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74067\n",
      "166693\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# separate train and test data\n",
    "data.registerTempTable(\"data\")\n",
    "sc = spark.sparkContext\n",
    "sql_sc = SQLContext(sc)\n",
    "\n",
    "data_train = sql_sc.sql(\"SELECT * from data where not isnan(relevance)\")\n",
    "data_test = sql_sc.sql(\"SELECT * from data where isnan(relevance)\")\n",
    "print(data_train.count())\n",
    "print(data_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### estimator: linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        prediction|\n",
      "+------------------+\n",
      "| 2.359582369248631|\n",
      "| 2.350930820360437|\n",
      "| 2.359582369248631|\n",
      "| 2.368233918136825|\n",
      "| 2.394188564801407|\n",
      "| 2.359582369248631|\n",
      "| 2.376885467025019|\n",
      "| 2.350930820360437|\n",
      "| 2.376885467025019|\n",
      "|2.3855370159132128|\n",
      "| 2.350930820360437|\n",
      "| 2.411491662577795|\n",
      "| 2.368233918136825|\n",
      "| 2.359582369248631|\n",
      "| 2.411491662577795|\n",
      "| 2.350930820360437|\n",
      "| 2.376885467025019|\n",
      "| 2.342279271472243|\n",
      "| 2.342279271472243|\n",
      "| 2.342279271472243|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "lr = LinearRegression(maxIter=10, regParam=0.01, elasticNetParam=0.8, labelCol='relevance')\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "model = pipeline.fit(data_train)\n",
    "predictions = model.transform(data_train)\n",
    "predictions.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.531697410278\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"relevance\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName:\"rmse\"})\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### estimator: random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        prediction|\n",
      "+------------------+\n",
      "|  2.33745428338531|\n",
      "|2.2880940387708226|\n",
      "|  2.33745428338531|\n",
      "|2.3505379442952723|\n",
      "|2.5045744198864583|\n",
      "|  2.33745428338531|\n",
      "|2.3917103789705707|\n",
      "|2.2880940387708226|\n",
      "|2.3917103789705707|\n",
      "|2.4544731738764645|\n",
      "|2.2880940387708226|\n",
      "| 2.511291698662927|\n",
      "|2.3505379442952723|\n",
      "|  2.33745428338531|\n",
      "| 2.511291698662927|\n",
      "|2.2880940387708226|\n",
      "|2.3917103789705707|\n",
      "| 2.037368113068499|\n",
      "| 2.037368113068499|\n",
      "| 2.037368113068499|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",labelCol='relevance', numTrees=11, maxDepth=5)\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "model = pipeline.fit(data_train)\n",
    "predictions = model.transform(data_train)\n",
    "predictions.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.521418945506\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"relevance\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName:\"rmse\"})\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### estimator: logistic regression (take as classification by times 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|double_prediction|\n",
      "+-----------------+\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "|              2.0|\n",
      "+-----------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.01, elasticNetParam=0.8, labelCol='int_relevance')\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "def toInt(score):\n",
    "    return int(score * 3)\n",
    "\n",
    "intUDF = udf(toInt, IntegerType())\n",
    "data_lr = data_train.withColumn(\"int_relevance\", intUDF('relevance'))\n",
    "model = pipeline.fit(data_lr)\n",
    "predictions = model.transform(data_train)\n",
    "\n",
    "def toDouble(score):\n",
    "    return score / 3\n",
    "\n",
    "doubleUDF = udf(toDouble, DoubleType())\n",
    "\n",
    "predictions = predictions.withColumn(\"double_prediction\", doubleUDF('prediction'))\n",
    "\n",
    "predictions.select('double_prediction').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.656481816907\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"relevance\", predictionCol=\"double_prediction\")\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName:\"rmse\"})\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### estimator: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# other estimators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
